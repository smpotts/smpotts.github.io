---
layout: page
title: Resume
cover-img: /assets/img/fiesta5.jpg
---

A curious and passionate self-learner who enjoys using technology and data to solve problems.

### Core Skills
**Engineering**: SQL, Python, Ruby on Rails, Docker, R, Kafka, Kubernetes, Terraform, Java 
**Data/ ETL**: dbt, Redshift, MySQL, Postgres, SQL Server, Jupyter Notebooks, Pandas, NumPy, Mage, Stitch, Census, Metabase, Looker, Google Analytics  
**Operating Systems**: Mac, Linux  
**AWS Services**: Redshift, Lambda, S3, RDS, CloudWatch, Kinesis Firehose, EC2, DMS  
**Project Management**: Git, Notion, Kanban, Asana, Agile methodologies, JIRA, Confluence  

### Education  
**Eastern University**  
Masters of Science in Data Science  
August 2024 - April 2026  

**University of Denver**  
Bachelor of Science in Computer Science  
Minors: Mathematics, Spanish  
January 2013 - June 2016 

### Work Experience
**Senior Data Engineer at ShorePoint Inc.**  |   *January 2025 – Present*  
* Engineered ETL pipelines in Python to ingest, transform, and enrich hardware and
software vulnerability data, supporting 3 million users across 22
federal agencies  
    * Processed data from 6 million assets, dynamically creating and streaming records
into 102 Kafka topics per agency, and applying ETL transformations and enriching the
data with vulnerability intelligence data  
    * Formulated structured document objects to index in Elasticsearch, enabling
downstream dashboards for real-time security insights  
* Migrated the project's containerized infrastructure from Docker Swarm to AWS EKS,
orchestrating seven Docker services within the data pipeline. Composed Kubernetes
manifests to enhance scalability, resilience, and maintainability  

**Senior Data & Analytics Engineer (Tech Lead) at eSpark Learning**   |   *August 2021 – December 2024*   
* Consumed data from a streaming events system using Kinesis Firehose to transmit
approximately 200,000 daily response events from OpenAI and Microsoft speech analysis to
the Redshift data warehouse  
    * Designed data models in dbt and produced analytical queries to assess product
eﬃcacy, enabling data-driven decisions that enhance learning outcomes for students
and support continuous improvement initiatives  
* Built +10 interactive dashboards in SQL to visualize student learning outcomes,
empowering sales teams with data-driven insights to demonstrate product eﬃcacy and drive
customer engagement  
* Utilized the open-source tool Mage for data orchestration, transferring data between the
internal Redshift warehouse and a Postgres instance for external customers, ensuring the
delivery of clean, reliable data to customer dashboards  
* Conducted data science analyses of product queries in Python using Jupyter Notebooks,
leveraging Pandas and NumPy enabling a better understanding of how the product
influenced student learning outcomes, engagement, and usage patterns  
* Developed and maintained AWS cloud infrastructure, implementing infrastructure as code
using Terraform, which enhanced system reliability and streamlined deployment processes  
* Contributed to data related projects within the core Ruby on Rails application,
enhancing functionality and optimizing performance  
    * Utilized the Redshift Data API to pipeline data aggregations into the Ruby on Rails
    application daily  
    * Engineered API integrations with key third-party data sources (Calendly, Salesforce,
    Intercom)  
    * Contributed to a cost-saving initiative by replacing a third-party service with a Ruby
    on Rails solution, resulting in annual savings of $300,000 and improved efficiency  

**Senior Software Engineer (Data Services) at Nasdaq**   |  *July 2018 – August 2021*   
* Developed order audit trail reports for clients and submitted them to FINRA  
    * Streamed daily trading activity, processing approximately 500 million records per day
using Java, Spring Boot, Kafka, and AWS, creating a cohesive timeline of linked orders  
* Generated risk management reports for clients on trading strategies, risk exposure, and
billing calculations using Microsoft SQL Server, processing up to 2 billion records  
    * Supported a cluster of 12 Windows servers (physical and virtual) to process data and
deliver 300 daily reports  
* Built and maintained a Postgres data warehouse with a REST API to analyze daily report
delivery metrics and ensure SLA accuracy  
* Leveraged the open-source project Poli to implement internal KPI dashboards for ad-hoc
reporting and data visualization  

**BI/ Data Warehouse Developer at Craftsy (acquired by NBCUniversal)** | *October 2017 – July 2018*  
* Expanded the data warehouse by pipelining data from additional sources and refining
architecture for business analysts and data consumers  
    * Configured Looker to retrieve data from over 100 tables in the data warehouse for
dashboards, segment analysis, and self-service reporting  
* Created a marketing forecast to calculate 30-day rolling averages for key metrics, costs,
and revenue across channels  
* Implemented API connections for marketing channels (YouTube, Facebook, Pinterest) to
gather ad engagement metrics and attribute costs  
* Established a new development environment and set development standards for the data
warehouse project  
    * Implemented version control with Git, created a JIRA task management workflow,
and set up a version-controlled password management tool using Python SOPS  

**Software Engineer (Data & BI) at Nasdaq** | *June 2015 – September 2017*  
* Designed a workflow engine for a new equities dark pool using Java and Spring,
ingesting data into secure Redshift client accounts per regulations  
* Launched a Business Intelligence project, creating a unified platform for five options and
three equities exchanges to deliver BI reports  
    * Established a Snowflake data model and constructed ETL processes with Pentaho
Data Integration  
    * Documented source-to-target data mappings and collaborated with business units
on reporting needs  
* Assisted in developing the Nasdaq Data Warehouse application using Java and Spring
    * Pipelined raw trading data into AWS under stringent regulatory requirement, ingesting
millions of records daily  
    * Archived Redshift data to Amazon S3 and Glacier for long-term storage, ensuring
WORM compliance with a retention period of 7 years  

**Software Engineering Intern at Forsythe Technologies** | *May 2014 – February 2015*  
* Wrote a Java program to translate client business processes and dependencies into BPMN
diagrams embedded in a webpage using JavaScript and AngularJS  
* Documented the development process using UML, class, sequence, and activity diagrams  

### Certifications
Certified Scrum Master | *May 2024 - 2026*   
AWS Certified Solutions Architect Associate | *April 2023 - 2026*    
AWS Certified Cloud Practitioner | *February 2023 - 2026*  
NASM Certified Nutrition Coach | *November 2023 - 2025*